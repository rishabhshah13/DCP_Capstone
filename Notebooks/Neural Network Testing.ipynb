{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8398dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "csv_file_name = '../Data/final_df.csv'\n",
    "df = pd.read_csv(csv_file_name, dtype={'Company Size': str})#, skiprows=lambda x: x % 2)\n",
    "\n",
    "hot_encode_columns = [\n",
    "    'Lead Job Title',\n",
    "    'Company Size',\n",
    "    'Company Industry',\n",
    "    'Company Li Company Type',\n",
    "    'Company Location Country Name',\n",
    "    'Email Status'\n",
    "]\n",
    "\n",
    "df_encoded = pd.DataFrame()\n",
    "\n",
    "current_year = datetime.datetime.now().year\n",
    "df_encoded['Years Since Company Founded'] = current_year - df['Company Founded In']\n",
    "\n",
    "# Convert Lead Years and Months to total months for position\n",
    "df_encoded['Total Months In Position'] = df['Lead Years In Position'] * 12 + df['Lead Months In Position']\n",
    "\n",
    "# Convert Lead Years and Months to total months for company tenure\n",
    "df_encoded['Total Months In Company'] = df['Lead Years In Company'] * 12 + df['Lead Months In Company']\n",
    "\n",
    "for column in hot_encode_columns+['Relevant', 'Company Followers', 'Company Des Relevant Score']:\n",
    "    df_encoded[column] = df[column]\n",
    "\n",
    "# Create a label encoder object\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "# Encode each column in hot_encode_columns with label encoding\n",
    "for column in hot_encode_columns:\n",
    "    # Create a new ordinal encoder object for each column\n",
    "    ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    \n",
    "    # Extract the column as a DataFrame with a single column\n",
    "    column_data = df_encoded[[column]]\n",
    "    \n",
    "    # Fit and transform the data using the encoder\n",
    "    encoded_column = ordinal_encoder.fit_transform(column_data)\n",
    "    \n",
    "    # Save the ordinal encoder to a file in the 'encoders' directory\n",
    "    encoder_filename = os.path.join('encoders', f'{column}_encoder.pkl')\n",
    "    with open(encoder_filename, 'wb') as file:\n",
    "        pickle.dump(ordinal_encoder, file)\n",
    "\n",
    "    # Replace the column in df_encoded with the encoded values\n",
    "    df_encoded[column] = encoded_column\n",
    "\n",
    "df_encoded.fillna(0, inplace=True)\n",
    "\n",
    "df_encoded['Relevant'] = df_encoded['Relevant'].replace(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c68692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df_encoded.drop(columns=['Relevant'])\n",
    "y = df_encoded['Relevant']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle class imbalance using upsampling of minority class (1)\n",
    "X_train_balanced, y_train_balanced = resample(X_train[y_train == 1],\n",
    "                                              y_train[y_train == 1],\n",
    "                                              replace=True,\n",
    "                                              n_samples=sum(y_train == 0),\n",
    "                                              random_state=39)\n",
    "\n",
    "X_train_balanced = pd.concat([X_train[y_train == 0], X_train_balanced])\n",
    "y_train_balanced = pd.concat([y_train[y_train == 0], y_train_balanced])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11ebccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[305 156]\n",
      " [ 10  52]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.66      0.79       461\n",
      "           1       0.25      0.84      0.39        62\n",
      "\n",
      "    accuracy                           0.68       523\n",
      "   macro avg       0.61      0.75      0.59       523\n",
      "weighted avg       0.88      0.68      0.74       523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "X_train_balanced_numeric = X_train_balanced.apply(pd.to_numeric, errors='coerce')\n",
    "X_train_balanced_numeric.dropna(inplace=True)\n",
    "X_train_array = X_train_balanced_numeric.values.astype(np.float32)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_array)\n",
    "y_train_tensor = torch.tensor(y_train_balanced.values, dtype=torch.int64)\n",
    "\n",
    "X_test_numeric = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "X_test_numeric.fillna(0, inplace=True)\n",
    "X_test_array = X_test_numeric.values.astype(np.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_array)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.int64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(11, 16)\n",
    "        self.layer2 = nn.Linear(16, 16)\n",
    "        self.layer3 = nn.Linear(16, 16)\n",
    "        self.layer4 = nn.Linear(16, 16)\n",
    "        self.layer5 = nn.Linear(16, 16)\n",
    "        self.layer6 = nn.Linear(16, 16)\n",
    "        self.layer7 = nn.Linear(16, 16)\n",
    "        self.output_layer = nn.Linear(16, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.relu(self.layer5(x))\n",
    "        x = self.relu(self.layer6(x))\n",
    "        x = self.relu(self.layer7(x))\n",
    "        x = self.softmax(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0004)\n",
    "\n",
    "for epoch in range(300):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/300, Loss: {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(X_test_tensor)\n",
    "    _, y_pred = torch.max(output, 1)\n",
    "\n",
    "y_pred_numpy = y_pred.numpy()\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_numpy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred_numpy)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'neural_network_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d821e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2613, 11)\n",
      "                       Company Name  Predicted_Relevancy\n",
      "0                           Evernow             0.997341\n",
      "1                           Portpro             0.995962\n",
      "2                           Rhumbix             0.995251\n",
      "3                             Round             0.994992\n",
      "4                    Skyloom Global             0.994003\n",
      "...                             ...                  ...\n",
      "2608  Bank Of America Merrill Lynch             0.000000\n",
      "2609              International Sos             0.000000\n",
      "2610                Bank Of America             0.000000\n",
      "2611            Cushman & Wakefield             0.000000\n",
      "2612                      Blackrock             0.000000\n",
      "\n",
      "[2608 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def load_and_infer(df):\n",
    "    # Load saved model\n",
    "    class NeuralNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(NeuralNetwork, self).__init__()\n",
    "            self.layer1 = nn.Linear(11, 16)\n",
    "            self.layer2 = nn.Linear(16, 16)\n",
    "            self.layer3 = nn.Linear(16, 16)\n",
    "            self.layer4 = nn.Linear(16, 16)\n",
    "            self.layer5 = nn.Linear(16, 16)\n",
    "            self.layer6 = nn.Linear(16, 16)\n",
    "            self.layer7 = nn.Linear(16, 16)\n",
    "            self.output_layer = nn.Linear(16, 2)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.layer1(x))\n",
    "            x = self.relu(self.layer2(x))\n",
    "            x = self.relu(self.layer3(x))\n",
    "            x = self.relu(self.layer4(x))\n",
    "            x = self.relu(self.layer5(x))\n",
    "            x = self.relu(self.layer6(x))\n",
    "            x = self.relu(self.layer7(x))\n",
    "            x = self.softmax(self.output_layer(x))\n",
    "            return x\n",
    "\n",
    "    model = NeuralNetwork()\n",
    "    model.load_state_dict(torch.load('neural_network_model.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess dataframe\n",
    "    df_encoded = pd.DataFrame()\n",
    "\n",
    "    current_year = datetime.datetime.now().year\n",
    "    df_encoded['Years Since Company Founded'] = current_year - df['Company Founded In']\n",
    "\n",
    "    df_encoded['Total Months In Position'] = df['Lead Years In Position'] * 12 + df['Lead Months In Position']\n",
    "\n",
    "    df_encoded['Total Months In Company'] = df['Lead Years In Company'] * 12 + df['Lead Months In Company']\n",
    "\n",
    "    for column in hot_encode_columns+['Company Followers', 'Company Des Relevant Score']:\n",
    "        df_encoded[column] = df[column]\n",
    "\n",
    "    for column in hot_encode_columns:\n",
    "        # Load the encoder\n",
    "        encoder_filename = f'encoders/{column}_encoder.pkl'  # Assuming the encoders are saved in a folder named 'encoders'\n",
    "        with open(encoder_filename, 'rb') as file:\n",
    "            encoder = pickle.load(file)\n",
    "        \n",
    "        # Encode the column in df_encoded\n",
    "        encoded_column = encoder.transform(df_encoded[[column]])\n",
    "        \n",
    "        # Replace the column in df_encoded with the encoded values\n",
    "        df_encoded[column] = encoded_column\n",
    "\n",
    "    df_encoded.fillna(0, inplace=True)\n",
    "\n",
    "    print(df_encoded.shape)\n",
    "\n",
    "    # Convert dataframe to tensor\n",
    "    df_numeric = df_encoded.apply(pd.to_numeric, errors='coerce')\n",
    "    df_numeric.dropna(inplace=True)\n",
    "    df_array = df_numeric.values.astype(np.float32)\n",
    "    df_tensor = torch.tensor(df_array)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(df_tensor)\n",
    "    \n",
    "    # Convert probabilities to raw probabilities\n",
    "    probabilities = output.numpy()\n",
    "\n",
    "    # Create a dataframe with Company Name and Predictions\n",
    "    company_names = df['Company Name'].iloc[df_numeric.index]\n",
    "    predictions_df = pd.DataFrame({'Company Name': company_names, 'Predicted_Relevancy': probabilities[:, 1]})\n",
    "\n",
    "    # Sort by Predicted_Relevancy\n",
    "    predictions_df = predictions_df.sort_values(by='Predicted_Relevancy', ascending=False)\n",
    "\n",
    "    predictions_df = predictions_df.reset_index(drop=True)\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df is the dataframe you want to perform inference on\n",
    "csv_file_name = '../Data/final_df.csv'\n",
    "df = pd.read_csv(csv_file_name, dtype={'Company Size': str})\n",
    "\n",
    "predictions_df = load_and_infer(df)\n",
    "predictions_df = predictions_df.dropna(subset=['Company Name'])\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee28e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
