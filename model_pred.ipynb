{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/SKde9xnq93NGB+5AU4Q8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabhshah13/DCP_Capstone/blob/main/model_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "from torch import cuda\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "ROQ5sXk0vRX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"des-classification-model\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer= BertTokenizerFast.from_pretrained(model_path)\n",
        "nlp= pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "9_AFelL-vTfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    \"\"\"\n",
        "    Predicts the class label for a given input text\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text for which the class label needs to be predicted.\n",
        "\n",
        "    Returns:\n",
        "        pred_label (str): The predicted class label.\n",
        "    \"\"\"\n",
        "    # Tokenize the input text and move tensors to the GPU if available\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Move the tensors to the same device as the model\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    # Ensure the model is in evaluation mode and on the correct device\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    # Get model output (logits)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    probs = outputs[0].softmax(1)\n",
        "    \"\"\" Explanation outputs: The BERT model returns a tuple containing the output logits (and possibly other elements depending on the model configuration). In this case, the output logits are the first element in the tuple, which is why we access it using outputs[0].\n",
        "\n",
        "    outputs[0]: This is a tensor containing the raw output logits for each class. The shape of the tensor is (batch_size, num_classes) where batch_size is the number of input samples (in this case, 1, as we are predicting for a single input text) and num_classes is the number of target classes.\n",
        "\n",
        "    softmax(1): The softmax function is applied along dimension 1 (the class dimension) to convert the raw logits into class probabilities. Softmax normalizes the logits so that they sum to 1, making them interpretable as probabilities. \"\"\"\n",
        "\n",
        "    # Get the index of the class with the highest probability\n",
        "    # argmax() finds the index of the maximum value in the tensor along a specified dimension.\n",
        "    # By default, if no dimension is specified, it returns the index of the maximum value in the flattened tensor.\n",
        "    pred_label_idx = probs.argmax()\n",
        "\n",
        "    # Now map the predicted class index to the actual class label\n",
        "    # Since pred_label_idx is a tensor containing a single value (the predicted class index),\n",
        "    # the .item() method is used to extract the value as a scalar\n",
        "    pred_label = model.config.id2label[pred_label_idx.item()]\n",
        "\n",
        "    return pred_label\n"
      ],
      "metadata": {
        "id": "tV5tAZJ8swsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.read_csv('/content/all_labelled_data_for_training.csv')"
      ],
      "metadata": {
        "id": "L6HdNl1Asxe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df['Company Li Description with null'] = final_df['Company Li Description'].copy()\n",
        "final_df['Company Li Description with null'] = final_df['Company Li Description with null'].fillna('null')\n",
        "final_df['Company Des Relevant Score'] = final_df['Company Li Description with null'].apply(predict)\n",
        "\n",
        "final_df.to_csv('/content/final_df.csv', index=False)"
      ],
      "metadata": {
        "id": "40ZZnMb8s1OF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}