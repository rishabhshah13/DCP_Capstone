{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = [\"Venture Capital\", \"legal practices\", \"consulting groups\", \"LLC (check for name)\"]\n",
    "good =  [\"Biotech\", \"crypto\", \"AI\", \"tech\", \"medtech\", \"pharmaceuticals\", \"med devices\", if a company has already raised venture capital funding, low amount of employees (look into funding round and number of employees), 18-24 months post last round]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query explainsion\n",
    "# Making vectors\n",
    "# Getting Score\n",
    "# classifying\n",
    "\n",
    "\n",
    "# Just using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms for words in the bad list:\n",
      "venture capital: {'adventure', 'uppercase', 'working_capital', 'embark', 'upper-case_letter', 'chapiter', 'capital_letter', 'Das_Kapital', 'venture', 'jeopardize', 'pretend', 'Capital', 'cap', 'majuscule', 'great', 'hazard', 'stake', 'speculation', 'Washington', 'capital', 'guess'}\n",
      "legal practices: {'rehearse', 'sound', 'practise', 'exercise', 'do', 'practice_session', 'use', 'effectual', 'legal', 'pattern', 'drill', 'apply', 'recitation', 'practice', 'praxis', 'commit'}\n",
      "consulting groups: {'confabulate', 'confer', 'consult', 'aggroup', 'confer_with', 'refer', 'mathematical_group', 'group', 'chemical_group', 'radical', 'grouping', 'look_up', 'confab'}\n",
      "LLC: set()\n",
      "\n",
      "Synonyms for words in the good list:\n",
      "biotech: {'biotechnology', 'biotech'}\n",
      "crypto: set()\n",
      "AI: {'AI', 'ai', 'artificial_insemination', 'three-toed_sloth', 'Army_Intelligence', 'artificial_intelligence', 'Bradypus_tridactylus'}\n",
      "tech: {'tech', 'technical_school'}\n",
      "medtech: set()\n",
      "pharmaceuticals: {'pharmaceutic', 'pharmaceutical'}\n",
      "medical devices: {'twist', 'gimmick', 'checkup', 'medical_checkup', 'health_check', 'device', 'medical_exam', 'aesculapian', 'medical_examination', 'devices', 'medical'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "bad_list = [\"venture capital\", \"legal practices\", \"consulting groups\", \"LLC\"]\n",
    "good_list = [\"biotech\", \"crypto\", \"AI\", \"tech\", \"medtech\", \"pharmaceuticals\", \"medical devices\"]\n",
    "\n",
    "def get_word_synonyms(word):\n",
    "    doc = nlp(word)\n",
    "    synonyms = set()\n",
    "    for token in doc:\n",
    "        synonyms |= get_synonyms(token.text)\n",
    "    return synonyms\n",
    "\n",
    "print(\"Synonyms for words in the bad list:\")\n",
    "for word in bad_list:\n",
    "    synonyms = get_word_synonyms(word)\n",
    "    print(f\"{word}: {synonyms}\")\n",
    "\n",
    "print(\"\\nSynonyms for words in the good list:\")\n",
    "for word in good_list:\n",
    "    synonyms = get_word_synonyms(word)\n",
    "    print(f\"{word}: {synonyms}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28526fda9d164fa19bd821fb16fb2e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rs659\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rs659\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84e6fcc343b4f5f850528192317d1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32348ead7f5c4a62be83efc8fce64214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f565b5f3874dc6b3a99882b93f4173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a4f805b75a48458aefc313a4f5a363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: venture capital\n",
      "Expanded query: venture capitalist angel investment startup funding\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# User query\n",
    "query = \"venture capital\"\n",
    "\n",
    "# Tokenize query\n",
    "tokenized_query = tokenizer.encode(query, return_tensors='pt')\n",
    "\n",
    "# Generate BERT embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokenized_query)\n",
    "    query_embedding = outputs[0].squeeze(0)  # CLS token embedding\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\"startup funding\", \"angel investment\", \"venture capitalist\", \"entrepreneurship\"]\n",
    "\n",
    "# Tokenize and embed corpus\n",
    "tokenized_corpus = tokenizer(corpus, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_corpus)\n",
    "    corpus_embeddings = outputs[0][:, 0, :].numpy()  # CLS token embeddings\n",
    "\n",
    "# Calculate cosine similarity between query embedding and corpus embeddings\n",
    "similarity_scores = cosine_similarity(query_embedding.numpy(), corpus_embeddings)\n",
    "\n",
    "# Sort corpus by similarity scores\n",
    "sorted_indices = np.argsort(similarity_scores[0])[::-1]\n",
    "\n",
    "# Retrieve top similar terms\n",
    "expanded_query = [corpus[i] for i in sorted_indices[:3]]  # Get top 3 similar terms\n",
    "expanded_query = ' '.join(expanded_query)\n",
    "\n",
    "print(\"Original query:\", query)\n",
    "print(\"Expanded query:\", expanded_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Public Good Pharma develops new low-cost therapies from system-wide cost savings of pharmaceutical payers  Our model employs 2 key mechanisms:   Interventional Pharmacoeconomic (IVPE) clinical trials generate guaranteed cost savings for payers by comparing in-use, expensive therapies to low-cost, safe and effective alternatives   +  Pay-For-Success Contracts (PFS) allow payers to incentivize regulatory approval for a low-cost alternative without taking risk. If IVPE trials are successful, PFS contracts guarantee supply.  \n",
      "Expanded sentence: [CLS] public good ph ##arm ##a develops new low - cost the ##ra ##pies from system - wide cost savings of pharmaceutical pay ##ers our model employs 2 key mechanisms : intervention ##al ph ##arm ##aco ##economic ( iv ##pe ) clinical trials generate guaranteed cost savings for pay ##ers by comparing in - use , expensive the ##ra ##pies to low - cost , safe and effective alternatives + pay - for - success contracts ( p ##fs ) allow pay ##ers to inc ##ent ##iv ##ize regulatory approval for a low - cost alternative without taking risk . if iv ##pe trials are successful , p ##fs contracts guarantee supply . [SEP] \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Original sentence\n",
    "# original_sentence = \"I'm interested in venture capital and legal practices.\"\n",
    "original_sentence = \"Public Good Pharma develops new low-cost therapies from system-wide cost savings of pharmaceutical payers  Our model employs 2 key mechanisms:   Interventional Pharmacoeconomic (IVPE) clinical trials generate guaranteed cost savings for payers by comparing in-use, expensive therapies to low-cost, safe and effective alternatives   +  Pay-For-Success Contracts (PFS) allow payers to incentivize regulatory approval for a low-cost alternative without taking risk. If IVPE trials are successful, PFS contracts guarantee supply.  \"\n",
    "\n",
    "# Lists of words to find similar terms for\n",
    "bad_list = [\"venture capital\", \"legal practices\", \"consulting groups\", \"LLC\"]\n",
    "good_list = [\"biotech\", \"crypto\", \"AI\", \"tech\", \"medtech\", \"pharmaceuticals\", \"medical devices\"]\n",
    "\n",
    "# Tokenize the original sentence\n",
    "tokenized_sentence = tokenizer.encode(original_sentence, return_tensors='pt')\n",
    "\n",
    "# Generate BERT embeddings for the tokens in the sentence\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokenized_sentence)\n",
    "    sentence_embeddings = outputs[0].squeeze(0)  # CLS token embedding\n",
    "\n",
    "# Function to get similar terms for a word from the provided list\n",
    "def get_similar_terms(word, word_list):\n",
    "    tokenized_words = tokenizer(word_list, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_words)\n",
    "        embeddings = outputs[0][:, 0, :].numpy()  # CLS token embeddings\n",
    "    similarity_scores = cosine_similarity(sentence_embeddings.numpy(), embeddings)\n",
    "    sorted_indices = np.argsort(similarity_scores[0])[::-1]\n",
    "    similar_words = [word_list[i] for i in sorted_indices[:3]]  # Get top 3 similar terms\n",
    "    return similar_words\n",
    "\n",
    "# Expand the original sentence by replacing matched words with similar terms\n",
    "expanded_sentence = \"\"\n",
    "for token, word in zip(tokenized_sentence[0], tokenizer.convert_ids_to_tokens(tokenized_sentence[0])):\n",
    "    if word in bad_list:\n",
    "        similar_terms = get_similar_terms(word, bad_list)\n",
    "        expanded_sentence += f\"{{{' '.join(similar_terms)}}} \"\n",
    "    elif word in good_list:\n",
    "        similar_terms = get_similar_terms(word, good_list)\n",
    "        expanded_sentence += f\"{{{' '.join(similar_terms)}}} \"\n",
    "    else:\n",
    "        expanded_sentence += word + \" \"\n",
    "\n",
    "print(\"Original sentence:\", original_sentence)\n",
    "print(\"Expanded sentence:\", expanded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: I'm interested in venture capital and legal practices.\n",
      "Expanded sentence: {[CLS]} {i} {'} {m} {interested} {in} {venture} {capital} {and} {legal} {practices} {.} {[SEP]} \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Original sentence\n",
    "original_sentence = \"I'm interested in venture capital and legal practices.\"\n",
    "\n",
    "# Lists of words to find similar terms for\n",
    "bad_list = [\"venture capital\", \"legal practices\", \"consulting groups\", \"LLC\"]\n",
    "good_list = [\"biotech\", \"crypto\", \"AI\", \"tech\", \"medtech\", \"pharmaceuticals\", \"medical devices\"]\n",
    "\n",
    "# Tokenize the original sentence\n",
    "tokenized_sentence = tokenizer.encode(original_sentence, return_tensors='pt')\n",
    "\n",
    "# Generate BERT embeddings for the tokens in the sentence\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokenized_sentence)\n",
    "    sentence_embeddings = outputs[0].squeeze(0)  # CLS token embedding\n",
    "\n",
    "# Function to get similar terms for a word from the provided list\n",
    "def get_similar_terms(word, word_list):\n",
    "    tokenized_words = tokenizer(word_list, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_words)\n",
    "        embeddings = outputs[0][:, 0, :].numpy()  # CLS token embeddings\n",
    "    similarity_scores = cosine_similarity(sentence_embeddings.numpy(), embeddings)\n",
    "    sorted_indices = np.argsort(similarity_scores[0])[::-1]\n",
    "    similar_words = [word_list[i] for i in sorted_indices[:3]]  # Get top 3 similar terms\n",
    "    return similar_words\n",
    "\n",
    "# Expand the original sentence by replacing matched words with similar terms\n",
    "expanded_sentence = \"\"\n",
    "for token, word in zip(tokenized_sentence[0], tokenizer.convert_ids_to_tokens(tokenized_sentence[0])):\n",
    "    if word in bad_list:\n",
    "        similar_terms = get_similar_terms(word, bad_list)\n",
    "        expanded_sentence += f\"{{{' '.join(similar_terms)}}} \"\n",
    "    elif word in good_list:\n",
    "        similar_terms = get_similar_terms(word, good_list)\n",
    "        expanded_sentence += f\"{{{' '.join(similar_terms)}}} \"\n",
    "    else:\n",
    "        expanded_sentence += f\"{{{word}}} \"\n",
    "\n",
    "print(\"Original sentence:\", original_sentence)\n",
    "print(\"Expanded sentence:\", expanded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: I'm interested in venture capital and legal practices.\n",
      "Expanded sentence: [CLS] i ' m interested in venture capital and legal practices . [SEP] \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Original sentence\n",
    "original_sentence = \"I'm interested in venture capital and legal practices.\"\n",
    "\n",
    "# Lists of words to find similar terms for\n",
    "bad_list = [\"venture capital\", \"legal practices\", \"consulting groups\", \"LLC\"]\n",
    "good_list = [\"biotech\", \"crypto\", \"AI\", \"tech\", \"medtech\", \"pharmaceuticals\", \"medical devices\"]\n",
    "\n",
    "# Tokenize the original sentence\n",
    "tokenized_sentence = tokenizer.encode(original_sentence, return_tensors='pt')\n",
    "\n",
    "# Generate BERT embeddings for the tokens in the sentence\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokenized_sentence)\n",
    "    sentence_embeddings = outputs[0].squeeze(0)  # CLS token embedding\n",
    "\n",
    "# Function to get similar terms for a word from the provided list\n",
    "def get_similar_terms(word, word_list):\n",
    "    tokenized_words = tokenizer(word_list, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_words)\n",
    "        embeddings = outputs[0][:, 0, :].numpy()  # CLS token embeddings\n",
    "    similarity_scores = cosine_similarity(sentence_embeddings.numpy(), embeddings)\n",
    "    sorted_indices = np.argsort(similarity_scores[0])[::-1]\n",
    "    similar_words = [word_list[i] for i in sorted_indices[:3]]  # Get top 3 similar terms\n",
    "    return similar_words\n",
    "\n",
    "# Expand the original sentence by replacing matched words with similar terms\n",
    "expanded_sentence = \"\"\n",
    "for token, word in zip(tokenized_sentence[0], tokenizer.convert_ids_to_tokens(tokenized_sentence[0])):\n",
    "    if word in bad_list:\n",
    "        similar_terms = get_similar_terms(word, bad_list)\n",
    "        expanded_sentence += f\"{{{ ' '.join(similar_terms) }}} \"\n",
    "    elif word in good_list:\n",
    "        similar_terms = get_similar_terms(word, good_list)\n",
    "        expanded_sentence += f\"{{{ ' '.join(similar_terms) }}} \"\n",
    "    else:\n",
    "        expanded_sentence += f\"{word} \"\n",
    "\n",
    "print(\"Original sentence:\", original_sentence)\n",
    "print(\"Expanded sentence:\", expanded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"sk2PFfoB2vewe6gki6tsm95yEsE5Ry6X\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
